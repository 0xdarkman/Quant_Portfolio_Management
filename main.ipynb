{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import os\n",
    "import seaborn as sns\n",
    "import args\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "from replay_memory import Memory, Transition\n",
    "from ounoise import OrnsteinUhlenbeckActionNoise as noise\n",
    "from env import trade_env\n",
    "import env\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "seed = 543\n",
    "memory = Memory(100000)\n",
    "noise_scale = 1.5\n",
    "final_noise_scale = 0.5\n",
    "worth = 20000\n",
    "cycle = 7\n",
    "rho = 3 # coefficient of risk aversion\n",
    "\n",
    "address = args.address\n",
    "writer = SummaryWriter(address + '/tensor')\n",
    "weights = address + '/weights'\n",
    "tensors = address + '/tensor'\n",
    "outputs = address + '/outputs'\n",
    "\n",
    "# for file in os.listdir(tensors):\n",
    "#     file = os.path.join(tensors,file)\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Actor, self).__init__()\n",
    "        self.affine1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.affine2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.value = nn.Linear(hidden_size, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.affine2(x)\n",
    "        x = self.ln2(x)\n",
    "        actions = F.softmax(self.value(x), dim=-1)\n",
    "\n",
    "        return actions\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Critic, self).__init__()\n",
    "        self.affine1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.affine2 = nn.Linear(action_space + hidden_size, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.value = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, actions):\n",
    "        x = self.affine1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = torch.cat((x, actions), 1)\n",
    "        x = self.affine2(x)\n",
    "        x = self.ln2(x)\n",
    "        q_value = self.value(F.relu(x))\n",
    "\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(seed)\n",
    "\n",
    "train_env = trade_env(env.train, worth, cycle, rho)\n",
    "test_env = trade_env(env.test, worth, cycle, rho)\n",
    "\n",
    "hidden_size =128\n",
    "action_space =  train_env.action_space\n",
    "state_space = train_env.state_space\n",
    "num_inputs = (action_space - 1) * state_space #  flatten info matrix; minus one since cash does not hold info\n",
    "\n",
    "critic = Critic(hidden_size, num_inputs, action_space)\n",
    "critic_target = Critic(hidden_size, num_inputs, action_space)\n",
    "actor = Actor(hidden_size, num_inputs, action_space)\n",
    "actor_target = Actor(hidden_size, num_inputs, action_space)\n",
    "actor_perturbed = Actor(hidden_size, num_inputs, action_space)\n",
    "\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "noise = noise(action_space)\n",
    "# noise.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.001\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_without_noise(state):  \n",
    "    action = actor(state)\n",
    "    action = action.detach().numpy()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select action with para noise on the last layer\n",
    "def select_action(state):  \n",
    "    \n",
    "    hard_update(actor_perturbed, actor)\n",
    "    actor_params = actor_perturbed.state_dict()\n",
    "    \n",
    "    param = actor_params['value.bias']\n",
    "    param += torch.tensor(noise()).float()\n",
    "        \n",
    "    action = actor_perturbed(state)\n",
    "    return action.detach().numpy()\n",
    "\n",
    "def select_action_with_para_noise(state):  \n",
    "    \n",
    "    hard_update(actor_perturbed, actor)\n",
    "    actor_params = actor_perturbed.state_dict()\n",
    "    \n",
    "    param = actor_params['value.bias']\n",
    "    param += torch.tensor(randn(action_space) * 1.8).float()\n",
    "        \n",
    "    action = actor_perturbed(state)\n",
    "    return action.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_update(critic_target, critic)\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_para():\n",
    "    transitions = memory.sample(128)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    state_batch = torch.stack(batch.state) \n",
    "    action_batch = torch.stack(batch.action)\n",
    "    utility_batch = torch.stack(batch.utility) \n",
    "    mask_batch = torch.stack(batch.mask)\n",
    "    next_state_batch = torch.stack(batch.next_state)\n",
    "    \n",
    "    q_batch = critic(state_batch, action_batch)\n",
    "    next_action_batch = actor_target(next_state_batch)\n",
    "    next_q_batch = utility_batch + gamma * mask_batch * critic_target(next_state_batch, next_action_batch)\n",
    "    \n",
    "    value_loss = F.mse_loss(q_batch, next_q_batch)\n",
    "    critic_optim.zero_grad()\n",
    "    value_loss.backward()\n",
    "    critic_optim.step()\n",
    "    \n",
    "    policy_loss = - critic(state_batch, actor(state_batch))\n",
    "    policy_loss = policy_loss.mean()\n",
    "    actor_optim.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    actor_optim.step()\n",
    "    \n",
    "    soft_update(actor_target, actor, tau)\n",
    "    soft_update(critic_target, critic, tau)\n",
    "    return value_loss, policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    ddpg_actor = os.path.join(path, 'ddpg_actor_weights')\n",
    "    ddpg_crtic = os.path.join(path, 'ddpg_critic_weights')\n",
    "    torch.save(actor.state_dict(), ddpg_actor)\n",
    "    torch.save(critic.state_dict(), ddpg_crtic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_tensor(state):\n",
    "    state = state.reshape(-1,  num_inputs)[0]\n",
    "    return torch.tensor(state).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = 0 \n",
    "running_reward = []\n",
    "\n",
    "for i_episode in range(5000):\n",
    "\n",
    "    abs_training_reward = 0\n",
    "    relative_training_reward = 0 \n",
    "    state = get_state_tensor(train_env.reset(worth))\n",
    "    \n",
    "    # dwindling noise \n",
    "    noise.scale = (noise_scale - final_noise_scale) * max(0, 3000-i_episode)/3000 + final_noise_scale\n",
    "    if train_env.date!=train_env.end_date:\n",
    "        for t in range(1, 100):\n",
    "\n",
    "            action = select_action(state)\n",
    "            \n",
    "            next_state, reward, SPX_reward, utility, done = train_env.step(action) # env.step() takes numpy array as inputs\n",
    "            \n",
    "#             print(action)\n",
    "#             print(train_env.worth)\n",
    "            \n",
    "            abs_training_reward += reward\n",
    "            relative_training_reward += (reward - SPX_reward)\n",
    "            action = torch.tensor(action).float()\n",
    "            mask = torch.tensor([not done]).float()\n",
    "            reward = torch.tensor([reward])\n",
    "            utility = torch.tensor([utility])\n",
    "            next_state = get_state_tensor(next_state)\n",
    "\n",
    "            # save to memory\n",
    "            memory.push(state, action, mask, utility, next_state)\n",
    "\n",
    "            if len(memory)>=128:\n",
    "                for _ in range(4):\n",
    "                    value_loss, policy_loss = update_para()\n",
    "                    writer.add_scalar('loss/value', value_loss, updates)\n",
    "                    writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                    updates += 1\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    #         print(abs_training_reward)\n",
    "        writer.add_scalar('training/abs reward', abs_training_reward, i_episode)\n",
    "        writer.add_scalar('training/relative reward', relative_training_reward, i_episode) \n",
    "\n",
    "    # test sample, evaluate model performance\n",
    "    abs_test_reward = 0\n",
    "    SPX_test_reward = 0\n",
    "    if i_episode % 1 == 0:\n",
    "        state = get_state_tensor(test_env.reset(worth))\n",
    "        if test_env.date!=test_env.end_date:\n",
    "            for t in range(1, 100):\n",
    "\n",
    "                action = select_action_without_noise(state)\n",
    "                next_state, reward, SPX_reward, utility, done = test_env.step(action) # env.step() takes numpy array as inputs\n",
    "                abs_test_reward += reward\n",
    "                SPX_test_reward += SPX_reward\n",
    "\n",
    "                next_state = get_state_tensor(next_state)\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break  \n",
    "            \n",
    "            relative_test_reward = abs_test_reward - SPX_test_reward\n",
    "            writer.add_scalar('test/abs reward', abs_test_reward, i_episode)\n",
    "            writer.add_scalar('test/relative reward', relative_test_reward, i_episode)\n",
    "\n",
    "            running_reward += [relative_test_reward]\n",
    "        if len(running_reward)>150 and np.median(running_reward[-100:]) > 70: # max reward: https://github.com/openai/gym/wiki/MountainCarContinuous-v0 \n",
    "\n",
    "            save_model(weights)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Test Models --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_weights = os.path.join(weights, 'ddpg_actor_weights')\n",
    "critic_weights =os.path.join(weights, 'ddpg_critic_weights')\n",
    "\n",
    "actor.load_state_dict(torch.load(actor_weights))\n",
    "critic.load_state_dict(torch.load(critic_weights))\n",
    "\n",
    "reward_list = []\n",
    "relative_reward_list = []\n",
    "for i_episode in range(5000):\n",
    "\n",
    "    state = get_state_tensor(test_env.reset(worth))    \n",
    "    episode_reward = 0\n",
    "    relative_episode_reward = 0\n",
    "    if test_env.date!=test_env.end_date:\n",
    "        for t in range(1, 500):\n",
    "\n",
    "            action = select_action_without_noise(state)\n",
    "#             print(test_env.share)\n",
    "#             print(test_env.date)\n",
    "            \n",
    "            next_state, reward, SPX_reward, utility, done = test_env.step(action) # env.step() takes numpy array as inputs\n",
    "            episode_reward += reward\n",
    "            relative_episode_reward += (reward - SPX_reward)\n",
    "\n",
    "            next_state = get_state_tensor(next_state)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break  \n",
    "#         print(\"Episode {} reward: {}\".format(str(test_env.date), str(episode_reward)))\n",
    "        \n",
    "    reward_list += [episode_reward]\n",
    "    relative_reward_list +=[relative_episode_reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Relative Reward':relative_reward_list,'Absolute Reward': reward_list})\n",
    "df.to_csv(outputs + '/test_sample_return.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(np.array(relative_reward_list), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPX_reward_list = np.array(reward_list) - np.array(relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(SPX_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(SPX_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(SPX_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(SPX_reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_weights = os.path.join(weights, 'ddpg_actor_weights')\n",
    "critic_weights =os.path.join(weights, 'ddpg_critic_weights')\n",
    "\n",
    "actor.load_state_dict(torch.load(actor_weights))\n",
    "critic.load_state_dict(torch.load(critic_weights))\n",
    "\n",
    "train_reward_list = []\n",
    "train_relative_reward_list = []\n",
    "for i_episode in range(5000):\n",
    "\n",
    "    state = get_state_tensor(train_env.reset(worth))    \n",
    "    episode_reward = 0\n",
    "    relative_episode_reward = 0\n",
    "    if train_env.date!=train_env.end_date:\n",
    "        for t in range(1, 100):\n",
    "\n",
    "            action = select_action(state)\n",
    "            print(train_env.share)\n",
    "            print(train_env.SPX_worth)\n",
    "            \n",
    "            next_state, reward, SPX_reward, done = train_env.step(action) # env.step() takes numpy array as inputs\n",
    "            episode_reward += reward\n",
    "            relative_episode_reward += (reward - SPX_reward)\n",
    "\n",
    "            next_state = get_state_tensor(next_state)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break  \n",
    "#         print(\"Episode {} reward: {}\".format(str(train_env.date), str(episode_reward)))\n",
    "        \n",
    "    train_reward_list += [episode_reward]\n",
    "    train_relative_reward_list +=[relative_episode_reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(train_relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_relative_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(np.array(reward_list), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(train_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(train_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
